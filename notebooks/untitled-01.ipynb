{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85973252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.core.problem import Problem\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "  def __init__(self, in_features: int, out_features: int):\n",
    "    self.l1 = nn.Linear(in_features, 64)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.l2 = nn.Linear(64, 32)\n",
    "    self.l3 = nn.Linear(32, out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.l1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.l2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.l3(x)\n",
    "    return x\n",
    "\n",
    "  def sample(self, state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a44f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(in_features: int, out_features: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, out_features)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5fbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_population = [\n",
    "  make_network(8, 16)\n",
    "  for _ in range(25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1467f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_encoded_actors = np.asarray([\n",
    "  parameters_to_vector(actor.parameters()).detach().cpu().numpy()\n",
    "  for actor in actor_population\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(actor_policy, env, replay_buffer=None):\n",
    "    \"\"\"evaluate fitness of actor's policy on an environment\"\"\"\n",
    "    total_reward = 0.0\n",
    "    observation, _ = env.reset(seed=42) # initial state\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action = actor_policy.sample(observation)\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if replay_buffer is not None:\n",
    "            replay_buffer.append((observation, action, reward, new_observation))\n",
    "        observation = new_observation\n",
    "    return total_reward\n",
    "\n",
    "class TheProblem(Problem):\n",
    "    def __init__(self, env, actors: list[nn.Module], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.actors = actors\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"X is the set of solutions, not just one solution\"\"\"\n",
    "        F = []\n",
    "        for i, x in enumerate(X):\n",
    "            actor = self.actors[i]\n",
    "            vector_to_parameters(torch.from_numpy(x), actor.parameters())\n",
    "            total_reward = run_episode(actor_policy=actor, env=self.env, replay_buffer=[])\n",
    "            F.append(-total_reward)\n",
    "        out[\"F\"] = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff0691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pso = PSO(\n",
    "  pop_size=len(vector_encoded_actors),\n",
    "  sampling=Population.new(X=vector_encoded_actors),\n",
    "  adaptive=False,\n",
    "  pertube_best=False,\n",
    "  seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "problem = TheProblem(env, actor_population, n_var=vector_encoded_actors[0].shape[0], xl=-1.0, xu=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b379085d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minimize\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m res = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m               \u001b[49m\u001b[43mpso\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m               \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_gen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m               \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m               \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/optimize.py:67\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(problem, algorithm, termination, copy_algorithm, copy_termination, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m     algorithm.setup(problem, **kwargs)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# actually execute the algorithm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m res = \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# store the deep copied algorithm in the result object\u001b[39;00m\n\u001b[32m     70\u001b[39m res.algorithm = algorithm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/algorithm.py:138\u001b[39m, in \u001b[36mAlgorithm.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_next():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/algorithm.py:158\u001b[39m, in \u001b[36mAlgorithm.next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# call the advance with them after evaluation\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infills \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfills\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance(infills=infills)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# if the algorithm does not follow the infill-advance scheme just call advance\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/evaluator.py:69\u001b[39m, in \u001b[36mEvaluator.eval\u001b[39m\u001b[34m(self, problem, pop, skip_already_evaluated, evaluate_values_of, count_evals, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# evaluate the solutions (if there are any)\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(I) > \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# do the actual evaluation - call the sub-function to set the corresponding values to the population\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# update the function evaluation counter\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_evals:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/evaluator.py:90\u001b[39m, in \u001b[36mEvaluator._eval\u001b[39m\u001b[34m(self, problem, pop, evaluate_values_of, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m X = pop.get(\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# call the problem to evaluate the solutions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m out = \u001b[43mproblem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluate_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# for each of the attributes set it to the problem\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m out.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/problem.py:257\u001b[39m, in \u001b[36mProblem.evaluate\u001b[39m\u001b[34m(self, X, return_values_of, return_as_dictionary, *args, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     only_single_value = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, np.ndarray))\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# this is where the actual evaluation takes place\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m _out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_of\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m out = {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _out.items():\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# copy it to a numpy array (it might be one of jax at this point)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/problem.py:299\u001b[39m, in \u001b[36mProblem.do\u001b[39m\u001b[34m(self, X, return_values_of, *args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate_elementwise(X, out, *args, **kwargs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# finally format the output dictionary\u001b[39;00m\n\u001b[32m    302\u001b[39m out = \u001b[38;5;28mself\u001b[39m._format_dict(out, \u001b[38;5;28mlen\u001b[39m(X), return_values_of)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/pymoo/core/problem.py:307\u001b[39m, in \u001b[36mProblem._evaluate_vectorized\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate_vectorized\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, out, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mTheProblem._evaluate\u001b[39m\u001b[34m(self, X, out, *args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m     actor = \u001b[38;5;28mself\u001b[39m.actors[i]\n\u001b[32m     24\u001b[39m     vector_to_parameters(torch.from_numpy(x), actor.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     total_reward = \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     F.append(-total_reward)\n\u001b[32m     27\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m] = F\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_episode\u001b[39m\u001b[34m(actor_policy, env, replay_buffer)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"evaluate fitness of actor's policy on an environment\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m total_reward = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m state = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m() \u001b[38;5;66;03m# initial state\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env.terminated():\n\u001b[32m      6\u001b[39m     action = actor_policy.sample(state)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ellipsis' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "from pymoo.optimize import minimize\n",
    "\n",
    "res = minimize(problem,\n",
    "               pso,\n",
    "               ('n_gen', 200),\n",
    "               seed=1,\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc03866",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_vector_encoded_actors = res.pop.get(\"X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
