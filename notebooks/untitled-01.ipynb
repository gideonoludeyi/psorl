{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85973252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.core.problem import Problem\n",
    "import gymnasium as gym\n",
    "from pymoo.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96458a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def sample(self, batch_size: int, random_state=None):\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        indices = rng.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states = zip(*[self.buffer[i] for i in indices])\n",
    "        return np.asarray(states), np.asarray(actions), np.asarray(rewards), np.asarray(next_states)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6825af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim: int, action_dim: int):\n",
    "    super(Actor, self).__init__()\n",
    "    self.l1 = nn.Linear(state_dim, 64)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.l2 = nn.Linear(64, 32)\n",
    "    self.l3 = nn.Linear(32, action_dim)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.l1(state)\n",
    "    x = self.relu(x)\n",
    "    x = self.l2(x)\n",
    "    x = self.relu(x)\n",
    "    action_probs = F.softmax(self.l3(x), dim=-1)\n",
    "    return action_probs\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim: int):\n",
    "    super(Critic, self).__init__()\n",
    "    self.l1 = nn.Linear(state_dim, 128)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.l2 = nn.Linear(128, 64)\n",
    "    self.l3 = nn.Linear(64, 1)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.l1(state)\n",
    "    x = self.relu(x)\n",
    "    x = self.l2(x)\n",
    "    x = self.relu(x)\n",
    "    value = self.l3(x)\n",
    "    return value\n",
    "\n",
    "class ActorCritic:\n",
    "  def __init__(self, state_dim: int, action_dim: int, device: torch.device):\n",
    "    self.actor = Actor(state_dim, action_dim).to(device)\n",
    "    self.critic = Critic(state_dim).to(device)\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, state):\n",
    "    return self.actor(state), self.critic(state)\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.FloatTensor(state)\n",
    "    action_probs = self.actor(state)\n",
    "    dist = torch.distributions.Categorical(action_probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.item(), log_prob\n",
    "  \n",
    "  def update_critic(self, replay_buffer: ReplayBuffer, gamma: float = 0.99, batch_size: int = 128, random_state=None):\n",
    "    states, actions, rewards, next_states = replay_buffer.sample(batch_size=batch_size, random_state=random_state)\n",
    "\n",
    "    states = torch.FloatTensor(states).to(self.device)\n",
    "    actions = torch.FloatTensor(actions).to(self.device)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "    next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "    \n",
    "    current_state_values = self.critic(states)\n",
    "    next_state_values = self.critic(next_states)\n",
    "    target_values = rewards + gamma * next_state_values\n",
    "  \n",
    "    loss = F.mse_loss(current_state_values, target_values)\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8903b21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(4), 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "action_dim = env.action_space.n\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim, observation_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d95eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\n",
    "    ActorCritic(state_dim=observation_dim, action_dim=action_dim, device=device)\n",
    "    for _ in range(25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22bdda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent: ActorCritic, env: gym.Env, replay_buffer: ReplayBuffer):\n",
    "    \"\"\"evaluate fitness of actor's policy on an environment\"\"\"\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    observation, _ = env.reset(seed=42) # initial state\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action, _ = agent.select_action(observation)\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        replay_buffer.push(observation, action, reward, new_observation)\n",
    "        observation = new_observation\n",
    "        steps += 1\n",
    "    return total_reward, steps\n",
    "\n",
    "class TheProblem(Problem):\n",
    "    def __init__(self, env, agents: list[ActorCritic], replay_buffer: ReplayBuffer, device: torch.device, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"X is the set of solutions, not just one solution\"\"\"\n",
    "        F = []\n",
    "        steps_list = []\n",
    "        for agent, x in zip(self.agents, X):\n",
    "            vector_to_parameters(torch.FloatTensor(x).to(self.device), agent.actor.parameters())\n",
    "            total_reward, steps = run_episode(agent=agent, env=self.env, replay_buffer=self.replay_buffer)\n",
    "            F.append(-total_reward)\n",
    "            steps_list.append(steps)\n",
    "        out[\"F\"] = F\n",
    "        out[\"steps\"] = steps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff0691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_encoded_actors = np.asarray([\n",
    "  parameters_to_vector(agent.actor.parameters()).detach().cpu().numpy()\n",
    "  for agent in agents\n",
    "])\n",
    "\n",
    "pso = PSO(\n",
    "  pop_size=len(vector_encoded_actors),\n",
    "  sampling=Population.new(X=vector_encoded_actors),\n",
    "  adaptive=False,\n",
    "  pertube_best=False,\n",
    "  seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eb8d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=10_000)\n",
    "\n",
    "problem = TheProblem(env=env, agents=agents, replay_buffer=replay_buffer, n_var=vector_encoded_actors[0].shape[0], xl=-1.0, xu=1.0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d8b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = minimize(problem,\n",
    "#                 pso,\n",
    "#                 ('n_gen', 200),\n",
    "#                 seed=1,\n",
    "#                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dd2729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(X, agents, env, replay_buffer, device):\n",
    "    F = []\n",
    "    steps_list = []\n",
    "    for x, agent in zip(X, agents):\n",
    "        vector_to_parameters(torch.FloatTensor(x).to(device), agent.actor.parameters())\n",
    "        total_reward, steps = run_episode(agent=agent, env=env, replay_buffer=replay_buffer)\n",
    "        F.append(-total_reward)\n",
    "        steps_list.append(steps)\n",
    "    return np.asarray(F), np.asarray(steps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c934e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pso.setup(problem)\n",
    "MAX_TIMESTEPS = 10_000\n",
    "t = 0\n",
    "while t < MAX_TIMESTEPS:\n",
    "    pop = pso.ask()\n",
    "    pop = pso.evaluator.eval(problem, pop, algorithm=pso)\n",
    "    t += np.sum(pop.get(\"steps\"))\n",
    "    pso.tell(pop)\n",
    "\n",
    "result = pso.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b379085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):\n",
    "#     res = minimize(problem,\n",
    "#                 pso,\n",
    "#                 ('n_gen', 10),\n",
    "#                 seed=1,\n",
    "#                 verbose=True)\n",
    "#     updated_vector_encoded_actors = res.pop.get(\"X\")\n",
    "#     for agent, vector_encoded_actor in zip(agents, updated_vector_encoded_actors):\n",
    "#         vector_to_parameters(torch.FloatTensor(vector_encoded_actor).to(device), agent.actor.parameters())\n",
    "#         agent.update_critic(replay_buffer=replay_buffer, batch_size=128)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
