{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85973252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.core.problem import Problem\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552fb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96458a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, action_log_prob, reward, next_state):\n",
    "        self.buffer.append((state, action, action_log_prob, reward, next_state))\n",
    "\n",
    "    def sample(self, batch_size: int, random_state=None):\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        indices = rng.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, action_log_probs, rewards, next_states = zip(\n",
    "            *[self.buffer[i] for i in indices]\n",
    "        )\n",
    "        return (\n",
    "            np.asarray(states),\n",
    "            np.asarray(actions),\n",
    "            action_log_probs,\n",
    "            np.asarray(rewards),\n",
    "            np.asarray(next_states),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6825af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.l1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        action_probs = F.softmax(self.l3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.l1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        value = self.l3(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, state_dim: int, action_dim: int, device: torch.device):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.critic = Critic(state_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.actor(state), self.critic(state)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        gamma: float = 0.99,\n",
    "        batch_size: int = 128,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        states, _, action_log_probs, rewards, next_states = replay_buffer.sample(\n",
    "            batch_size=batch_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        states = torch.FloatTensor(states, device=self.device)\n",
    "        action_log_probs = torch.stack(action_log_probs).to(self.device).unsqueeze(1)\n",
    "        # action_log_probs = torch.FloatTensor(action_log_probs, device=self.device)\n",
    "        rewards = torch.FloatTensor(rewards, device=self.device).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states, device=self.device)\n",
    "\n",
    "        # --- Critic Update ---\n",
    "        current_state_values = self.critic(states)\n",
    "        # Get the next state value prediction from the critic\n",
    "        # For the target, we don't want gradients to flow through next_state_value\n",
    "        # if the episode is done, next_state_value is 0 (no future rewards)\n",
    "        next_state_values = self.critic(next_states)\n",
    "        # Calculate the TD target (R + gamma * V(s'))\n",
    "        target_values = rewards + gamma * next_state_values\n",
    "\n",
    "        # Critic loss: Mean Squared Error between predicted value and TD target\n",
    "        critic_loss = F.mse_loss(current_state_values, target_values)\n",
    "        # Perform backpropagation for the critic\n",
    "        self.critic_optimizer.zero_grad()  # Clear previous gradients\n",
    "        critic_loss.backward()  # Compute gradients\n",
    "        self.critic_optimizer.step()  # Update critic network parameters\n",
    "\n",
    "        # --- Actor Update ---\n",
    "        # Calculate the Advantage (TD Error): TD_target - V(s)\n",
    "        # It's crucial to detach target_values and current_state_values here to prevent\n",
    "        # gradients from flowing back into the Critic network during the Actor's update.\n",
    "        # The Actor's update should only depend on the value estimate, not train the critic.\n",
    "        advantages = (target_values - current_state_values).detach()\n",
    "\n",
    "        # Actor loss: Negative log-probability weighted by the advantage\n",
    "        # We want to maximize expected reward, so we minimize negative expected reward.\n",
    "        actor_loss = -action_log_probs * advantages  # .unsqueeze(0)\n",
    "        # Perform backpropagation for the actor\n",
    "        self.actor_optimizer.zero_grad()  # Clear previous gradients\n",
    "        # see the following thread for the reason behind .mean().backward()\n",
    "        # https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152\n",
    "        print(action_log_probs.size())\n",
    "        print(advantages.size())\n",
    "        mean_loss = actor_loss.sum(dim=0) / batch_size\n",
    "        mean_loss.backward()  # Compute gradients # FIXME\n",
    "        self.actor_optimizer.step()  # Update actor network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8903b21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(4), 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "action_dim = env.action_space.n\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim, observation_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d95eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\n",
    "    ActorCritic(state_dim=observation_dim, action_dim=action_dim, device=device)\n",
    "    for _ in range(25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22bdda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent: ActorCritic, env: gym.Env, replay_buffer: ReplayBuffer):\n",
    "    \"\"\"evaluate fitness of actor's policy on an environment\"\"\"\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    observation, _ = env.reset(seed=42)  # initial state\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        action, log_prob = agent.select_action(observation)\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        replay_buffer.push(observation, action, log_prob, reward, new_observation)\n",
    "        observation = new_observation\n",
    "        steps += 1\n",
    "    return total_reward, steps\n",
    "\n",
    "\n",
    "class TheProblem(Problem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        agents: list[ActorCritic],\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        device: torch.device,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"X is the set of solutions, not just one solution\"\"\"\n",
    "        F = []\n",
    "        steps_list = []\n",
    "        for agent, x in zip(self.agents, X):\n",
    "            vector_to_parameters(\n",
    "                torch.FloatTensor(x).to(self.device), agent.actor.parameters()\n",
    "            )\n",
    "            total_reward, steps = run_episode(\n",
    "                agent=agent, env=self.env, replay_buffer=self.replay_buffer\n",
    "            )\n",
    "            F.append(-total_reward)\n",
    "            steps_list.append(steps)\n",
    "        out[\"F\"] = F\n",
    "        out[\"steps\"] = steps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff0691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_encoded_actors = np.asarray(\n",
    "    [\n",
    "        parameters_to_vector(agent.actor.parameters()).detach().cpu().numpy()\n",
    "        for agent in agents\n",
    "    ]\n",
    ")\n",
    "\n",
    "pso = PSO(\n",
    "    pop_size=len(vector_encoded_actors),\n",
    "    sampling=Population.new(X=vector_encoded_actors),\n",
    "    adaptive=False,\n",
    "    pertube_best=False,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eb8d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=10_000)\n",
    "\n",
    "problem = TheProblem(\n",
    "    env=env,\n",
    "    agents=agents,\n",
    "    replay_buffer=replay_buffer,\n",
    "    n_var=vector_encoded_actors[0].shape[0],\n",
    "    xl=-2.0,\n",
    "    xu=2.0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================\n",
      "n_gen  |  n_eval  |    f     |    S    |    w    |    c1    |    c2    |     f_avg     |     f_min    \n",
      "======================================================================================================\n",
      "     1 |       25 |        - |       - |  0.9000 |  2.00000 |  2.00000 |  2.074611E+02 |  5.898743E+01\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m     agent = agents[i]\n\u001b[32m     30\u001b[39m     vector_to_parameters(torch.FloatTensor(actor_params).to(device), agent.actor.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m stage == \u001b[32m2\u001b[39m: \u001b[38;5;66;03m# (optimize Pb via RL)\u001b[39;00m\n\u001b[32m     33\u001b[39m     actor_params = pop[b].X\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mActorCritic.update\u001b[39m\u001b[34m(self, replay_buffer, gamma, batch_size, random_state)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(advantages.size())\n\u001b[32m     93\u001b[39m mean_loss = actor_loss.sum(dim=\u001b[32m0\u001b[39m) / batch_size\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mmean_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m             \u001b[38;5;66;03m# Compute gradients # FIXME\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.actor_optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/psorl/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "pso.setup(problem, verbose=True)\n",
    "MAX_TIMESTEPS = 10_000 * 100\n",
    "EXPLORATION_RATIO = 0.25\n",
    "L = np.zeros(pso.pop_size)\n",
    "B = np.zeros(pso.pop_size)\n",
    "t, e, b = (0, 0, 0)\n",
    "pop = pso.ask()\n",
    "pop = pso.evaluator.eval(problem, pop, algorithm=pso)\n",
    "while t < MAX_TIMESTEPS:\n",
    "    stage = 1 if (t < MAX_TIMESTEPS * EXPLORATION_RATIO) else 2\n",
    "    pso.tell(pop)\n",
    "    index_list = ([e] * pso.pop_size) + list(range(pso.pop_size))\n",
    "    for i in index_list:\n",
    "        fitness, steps = pso.evaluator.eval(problem, pop[i], algorithm=pso).get(\n",
    "            \"F\", \"steps\"\n",
    "        )\n",
    "        fitness = fitness[0]\n",
    "        t += steps\n",
    "        L[i] += steps\n",
    "        if fitness > B[i]:\n",
    "            B[i] = fitness\n",
    "            L[i] = 0\n",
    "            e = i\n",
    "        if B[i] > np.max(B):\n",
    "            if stage == 1:\n",
    "                b = i\n",
    "            elif stage == 2:\n",
    "                pop[b] = pop[i].copy()\n",
    "        if stage == 1:  # (optimize Pi by via RL)\n",
    "            actor_params = pop[i].X\n",
    "            agent = agents[i]\n",
    "            vector_to_parameters(\n",
    "                torch.FloatTensor(actor_params).to(device), agent.actor.parameters()\n",
    "            )\n",
    "            agent.update(replay_buffer=replay_buffer, batch_size=128)\n",
    "            pop[i].set(\"X\", parameters_to_vector(agent.actor.parameters()))\n",
    "        elif stage == 2:  # (optimize Pb via RL)\n",
    "            actor_params = pop[b].X\n",
    "            agent = agents[b]\n",
    "            vector_to_parameters(\n",
    "                torch.FloatTensor(actor_params).to(device), agent.actor.parameters()\n",
    "            )\n",
    "            agent.update(replay_buffer=replay_buffer, batch_size=128)\n",
    "            pop[b].set(\"X\", parameters_to_vector(agent.actor.parameters()))\n",
    "    if L[e] > L[b] or stage == 2:\n",
    "        e = b\n",
    "    # ask-and-tell is inverted because `ask()` does the PSO update\n",
    "    # which happens at the end of the while-loop according to Two-Stage ERL (TERL)\n",
    "    pop = pso.ask()\n",
    "\n",
    "result = pso.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0756da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.98743138])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymoo.optimize import minimize\n",
    "#\n",
    "# res = minimize(problem,\n",
    "#                 pso,\n",
    "#                 ('n_gen', 200),\n",
    "#                 seed=1,\n",
    "#                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):\n",
    "#     res = minimize(problem,\n",
    "#                 pso,\n",
    "#                 ('n_gen', 10),\n",
    "#                 seed=1,\n",
    "#                 verbose=True)\n",
    "#     updated_vector_encoded_actors = res.pop.get(\"X\")\n",
    "#     for agent, vector_encoded_actor in zip(agents, updated_vector_encoded_actors):\n",
    "#         vector_to_parameters(torch.FloatTensor(vector_encoded_actor).to(device), agent.actor.parameters())\n",
    "#         agent.update(replay_buffer=replay_buffer, batch_size=128)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psorl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
